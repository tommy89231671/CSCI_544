Complete this document and upload along with your prediction results and your code.

### Method Name ###
Training our own dataset on  a fine-tuning BERT model.

### Sentence pair encoder ###
It is a transformer bi-encoder, based on BERT pretraied on Wikipedia and BooksCorpus.

### Training & Development ###
At training and dev dataset, I concatenate two sentences with proper tag ([CLS] and [SEP]). Then tokenize it to ids array. The last layer (Dense) of the model would output 0 or 1. Fine-tune the weight for 4 epcohs with adamw optimizer, 32 batch_size and 3e-5 learning rate.

### Other methods ###
A light BERT

### Packages ###
- Tensorflow
- numpy
