{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TisVoo15NpOO"
      },
      "source": [
        "### Read training, dev and unlabeled test data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRElk7aeNsXf",
        "outputId": "3e4f8269-58ee-40cf-ad70-2fb7335cbc10"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a5hITsNNpOQ"
      },
      "source": [
        "The following provides a starting code (Python 3) of how to read the labeled training and dev sentence pairs, and unlabeled test sentence pairs, into lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JYkX-r-vNpOR"
      },
      "outputs": [],
      "source": [
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZKJQdhGzNpOR"
      },
      "outputs": [],
      "source": [
        "train, dev, test = [], [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BbV-0tBNpOR",
        "outputId": "72ff5612-d9e9-4769-b3d7-35ea8f553f58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5983\n",
            "[['Sometimes do exercise.', 'A person typically desire healthy life.', '1'], ['Who eats junk foods.', 'A person typically desire healthy life.', '0'], ['A person is sick.', 'A person typically desire healthy life.', '1']]\n"
          ]
        }
      ],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/data/pnli_train.csv', encoding='utf-8') as fp:\n",
        "    csvreader = csv.reader(fp)\n",
        "    for x in csvreader:\n",
        "        # x[2] will be the label (0 or 1). x[0] and x[1] will be the sentence pairs.\n",
        "        train.append(x)\n",
        "print (len(train))\n",
        "print (train[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhbAp5YbNpOS",
        "outputId": "87d8fc8b-618b-4a88-a574-e2626636ca82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1055\n",
            "[['A person is looking for accuracy.', 'A person typically desires accurate results.', '1'], ['A person does not care for accuracy.', 'A person typically desires accurate results.', '0'], ['The person double checks their data.', 'A person typically desires accurate results.', '1']]\n"
          ]
        }
      ],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/data/pnli_dev.csv', encoding='utf-8') as fp:\n",
        "    csvreader = csv.reader(fp)\n",
        "    for x in csvreader:\n",
        "        # x[2] will be the label (0 or 1). x[0] and x[1] will be the sentence pairs.\n",
        "        dev.append(x)\n",
        "print (len(dev))\n",
        "print (dev[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIYlXH5vNpOS",
        "outputId": "4e251f29-9eea-42b6-faf0-5696eb554c05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4850\n",
            "[['The people want to have a romantic and pleasant feel.', 'People typically does desire to smell violets.'], ['The contract is to buy products from you.', 'Getting contract typically cause to make money or spend money.'], ['Train station is closed.', 'Line can typically be used to move train along tracks.']]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/data/pnli_test_unlabeled.csv', encoding='utf-8') as fp:\n",
        "    csvreader = csv.reader(fp)\n",
        "    for x in csvreader:\n",
        "        # x[0] and x[1] will be the sentence pairs.\n",
        "        test.append(x)\n",
        "print (len(test))\n",
        "print (test[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN3YK9gPNpOT"
      },
      "source": [
        "### Main Code Body"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MOR-jBYNpOT"
      },
      "source": [
        "You may choose to experiment with different methods using your program. However, you need to embed the training and inference processes at here. We will use your prediction on the unlabeled test data to grade, while checking this part to understand how your method has produced the predictions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U \"tensorflow-text==2.8.*\"\n",
        "!pip install -q tf-models-official==2.4.0\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "from official.modeling import tf_utils\n",
        "from official import nlp\n",
        "from official.nlp import bert\n",
        "# Load the required submodules\n",
        "import official.nlp.optimization\n",
        "import official.nlp.bert.bert_models\n",
        "import official.nlp.bert.configs\n",
        "import official.nlp.bert.run_classifier\n",
        "import official.nlp.bert.tokenization\n",
        "import official.nlp.data.classifier_data_lib\n",
        "import official.nlp.modeling.losses\n",
        "import official.nlp.modeling.models\n",
        "import official.nlp.modeling.networks\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer"
      ],
      "metadata": {
        "id": "KtLvJoD-RE2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5962a564-53c7-4a37-9ac0-cc54e8138dd9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.9 MB 14.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 84.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 14.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 66.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47.8 MB 101 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 64.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 237 kB 75.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 82.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 53.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 11.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.2 MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/albert_en_base/2'\n",
        "# tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/albert_en_preprocess/3'\n",
        "\n",
        "tfhub_handle_encoder = 'https://tfhub.dev/google/experts/bert/wiki_books/2'\n",
        "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
        "\n",
        "# bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "# bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
        "def build_classifier_model():\n",
        "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text_input')\n",
        "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "    encoder_inputs = preprocessing_layer(text_input)\n",
        "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "    outputs = encoder(encoder_inputs)\n",
        "    net = outputs['pooled_output']\n",
        "    net = tf.keras.layers.Dropout(0.1)(net)\n",
        "    net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "    return tf.keras.Model(text_input, net)"
      ],
      "metadata": {
        "id": "EG0T6RcRdwVK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(data):\n",
        "  tmp_sen_list = []\n",
        "  tmp_label_list = []\n",
        "  for sen in data:\n",
        "    tmp_sen_list.append(\"[CLS] \"+sen[0]+\" [SEP] \"+sen[1]+\" [SEP]\")\n",
        "    tmp_label_list.append(float(sen[2]))\n",
        "  sentences_dataset = tf.data.Dataset.from_tensor_slices(tmp_sen_list)\n",
        "  label_dataset = tf.data.Dataset.from_tensor_slices(tmp_label_list)\n",
        "  return tf.data.Dataset.zip((sentences_dataset,label_dataset))\n",
        "\n",
        "train_dataset = build_dataset(train)\n",
        "train_dataset = train_dataset.batch(batch_size = 32,name = \"train_dataset\")\n",
        "\n",
        "dev_dataset = build_dataset(dev)\n",
        "dev_dataset = dev_dataset.batch(batch_size = 32,name = \"dev_dataset\")\n",
        "\n",
        "def build_test_dataset(data):\n",
        "  tmp_sen_list = []\n",
        "  for sen in data:\n",
        "    tmp_sen_list.append(\"[CLS] \"+sen[0]+\" [SEP] \"+sen[1]+\" [SEP]\")\n",
        "  return tf.data.Dataset.from_tensor_slices(tmp_sen_list)\n",
        "test_dataset = build_test_dataset(test)\n",
        "test_dataset = test_dataset.batch(batch_size = 32,name = \"test_dataset\")\n"
      ],
      "metadata": {
        "id": "c3pYjtDYZLPP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model = build_classifier_model()"
      ],
      "metadata": {
        "id": "5xMUASzLef0p"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ],
      "metadata": {
        "id": "gvfV1HWHqAbZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model(tf.constant([\"[CLS] \"+dev[0][0]+\" [SEP] \"+dev[0][1]+\" [SEP]\"])).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivbgEsq98ae_",
        "outputId": "80575a77-61c7-4c07-ca11-c6c963784a91"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.72539896]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 4\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ],
      "metadata": {
        "id": "p_Mw8VqGqEGP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ],
      "metadata": {
        "id": "_wwFQWQzl6FS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "history = classifier_model.fit(x=train_dataset,\n",
        "                               validation_data = dev_dataset,\n",
        "                               epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uit0C57XqTYP",
        "outputId": "cfd21b7e-dddf-496e-b8eb-35f7a0d1137c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with https://tfhub.dev/google/experts/bert/wiki_books/2\n",
            "Epoch 1/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "187/187 [==============================] - 182s 890ms/step - loss: 0.6239 - binary_accuracy: 0.6356 - val_loss: 0.5141 - val_binary_accuracy: 0.7877\n",
            "Epoch 2/4\n",
            "187/187 [==============================] - 169s 901ms/step - loss: 0.4345 - binary_accuracy: 0.8098 - val_loss: 0.4170 - val_binary_accuracy: 0.8227\n",
            "Epoch 3/4\n",
            "187/187 [==============================] - 169s 906ms/step - loss: 0.3395 - binary_accuracy: 0.8629 - val_loss: 0.4351 - val_binary_accuracy: 0.8227\n",
            "Epoch 4/4\n",
            "187/187 [==============================] - 169s 905ms/step - loss: 0.2691 - binary_accuracy: 0.8920 - val_loss: 0.4447 - val_binary_accuracy: 0.8408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.save_weights('/content/drive/MyDrive/Colab Notebooks/hw4_weights/hw4_epoch4_wiki_bert.ckpt')"
      ],
      "metadata": {
        "id": "DwEMHdV8qwLu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_result = classifier_model.predict(test_dataset)"
      ],
      "metadata": {
        "id": "pJmgzgYG0dhm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eventually, results need to be a list of 2028 0 or 1's\n",
        "results = []\n",
        "for a in predict_result:\n",
        "  results.append(round(a[0]))\n"
      ],
      "metadata": {
        "id": "l7qFexpD2Dj2"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reload_model = build_classifier_model()\n",
        "# reload_model.load_weights('/content/drive/MyDrive/Colab Notebooks/hw4_weights/hw4_1.ckpt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tPUcjqmyUxN",
        "outputId": "e2bf96d0-f89b-41be-e5e0-cf2da274fca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbd82e39ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGwuBXKsNpOT"
      },
      "source": [
        "### Output Prediction Result File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJWOopC5NpOU"
      },
      "source": [
        "You will need to submit a prediction result file. It should have 2028 lines, every line should be either 0 or 1, which is your model's prediction on the respective test set instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3spz1BGxNpOU"
      },
      "outputs": [],
      "source": [
        "# suppose you had your model's predictions on the 2028 test cases read from test_enc_unlabeled.tsv, and \n",
        "#those results are in the list called 'results'\n",
        "assert (len(results) == 4850)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "oQ9TwzkcNpOU"
      },
      "outputs": [],
      "source": [
        "# make sure the results are not float numbers, but intergers 0 and 1\n",
        "results = [int(x) for x in results]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "i26osoYsNpOU"
      },
      "outputs": [],
      "source": [
        "# write your prediction results to 'upload_predictions.txt' and upload that later\n",
        "with open('upload_predictions.txt', 'w', encoding = 'utf-8') as fp:\n",
        "    for x in results:\n",
        "        fp.write(str(x) + '\\n')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}