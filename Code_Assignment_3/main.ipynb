{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read training, dev and unlabeled test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following provides a starting code (Python 3) of how to read the labeled training and dev cipher text, and unlabeled test cipher text, into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in open('./train_enc.tsv', encoding='utf-8'):\n",
    "    x = x.rstrip('\\n\\r').split('\\t')\n",
    "    # x[0] will be the label (0 or 1), and x[1] will be the ciphertext sentence.\n",
    "    x[0] = int(x[0]) \n",
    "    train.append(x)\n",
    "# print (len(train))\n",
    "# print (train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in open('./dev_enc.tsv', encoding='utf-8'):\n",
    "    x = x.rstrip('\\n\\r').split('\\t')\n",
    "    # x[0] will be the label (0 or 1), and x[1] will be the ciphertext sentence.\n",
    "    x[0] = int(x[0]) \n",
    "    dev.append(x)\n",
    "# print (len(dev))\n",
    "# print (dev[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different from 'train' and 'dev' that are both list of tuples, 'test' will be just a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in open('./test_enc_unlabeled.tsv', encoding='utf-8'):\n",
    "    x = x.rstrip('\\n\\r')\n",
    "    test.append(x)\n",
    "# print (len(test))\n",
    "# print (test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can split every sentence into lists of words by white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = [[x[0], x[1].split(' ')] for x in train]\n",
    "dev_split = [[x[0], x[1].split(' ')] for x in dev]\n",
    "test_split = [[x.split(' ')] for x in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code Body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may choose to experiment with different methods using your program. However, you need to embed the training and inference processes at here. We will use your prediction on the unlabeled test data to grade, while checking this part to understand how your method has produced the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "sen_list=[]\n",
    "word_set=set()\n",
    "word_counter=0\n",
    "for sentence in train_split:\n",
    "    sen_list.append(sentence[1])\n",
    "    for word in sentence[1]:\n",
    "        word_counter+=1\n",
    "        word_set.add(word)\n",
    "# print(word_counter)\n",
    "# print(len(word_set))\n",
    "\n",
    "wvmodel = Word2Vec(sen_list, min_count=1,vector_size = 100, epochs = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g2/ff5n7hb15tv2bvzy9sx0jcpr0000gn/T/ipykernel_29803/1564043174.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/pytorch/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  train_dataset.append(torch.tensor(word_embedding, dtype=torch.float))\n"
     ]
    }
   ],
   "source": [
    "seq_len = []\n",
    "label_list = []\n",
    "train_dataset = []\n",
    "max_len = 100\n",
    "for a in train_split:\n",
    "    word_embedding=[]\n",
    "    label_list.append(a[0])\n",
    "    seq_len.append(len(a[1]))\n",
    "    for word in a[1]:\n",
    "        word_embedding.append(wvmodel.wv[word])\n",
    "\n",
    "    train_dataset.append(torch.tensor(word_embedding, dtype=torch.float))\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    train_dataset[i] = F.pad(train_dataset[i], pad=(0, 0, 0, max_len - train_dataset[i].shape[0]))\n",
    "\n",
    "dev_seq_len = []\n",
    "dev_label_list = []\n",
    "dev_dataset = []\n",
    "for a in dev_split:\n",
    "    word_embedding=[]\n",
    "    dev_label_list.append(a[0])\n",
    "    dev_seq_len.append(len(a[1]))\n",
    "    for word in a[1]:\n",
    "        if word not in wvmodel.wv:\n",
    "            word_embedding.append([0]*100)\n",
    "        else:\n",
    "            word_embedding.append(wvmodel.wv[word])\n",
    "\n",
    "    dev_dataset.append(torch.tensor(word_embedding, dtype=torch.float))\n",
    "\n",
    "for i in range(len(dev_dataset)):\n",
    "    dev_dataset[i] = F.pad(dev_dataset[i], pad=(0, 0, 0, max_len - dev_dataset[i].shape[0]))\n",
    "\n",
    "test_dataset = []\n",
    "for a in test_split:\n",
    "    word_embedding=[]\n",
    "    for word in a[0]:\n",
    "        if word not in wvmodel.wv:\n",
    "            word_embedding.append([0]*100)\n",
    "        else:\n",
    "            word_embedding.append(wvmodel.wv[word])\n",
    "\n",
    "    test_dataset.append(torch.tensor(word_embedding, dtype=torch.float))\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    test_dataset[i] = F.pad(test_dataset[i], pad=(0, 0, 0, max_len - test_dataset[i].shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING_DIM = 100\n",
    "# HIDDEN_DIM = 5\n",
    "# class cipher_LSTM(nn.Module):\n",
    "\n",
    "#     def __init__(self, embedding_dim, hidden_dim,max_len):\n",
    "#         super(cipher_LSTM, self).__init__()\n",
    "#         # self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "#         # self.multihead_attn = nn.MultiheadAttention(embedding_dim, hidden_dim)\n",
    "#         self.linear1 = nn.Linear(hidden_dim, 1)\n",
    "#         self.linear2 = nn.Linear(max_len, 1)\n",
    "\n",
    "#     def forward(self, sentence):\n",
    "#         # embeds = self.word_embeddings(sentence)\n",
    "#         lstm_out, _ = self.lstm(sentence.view(len(sentence), 1, -1))\n",
    "#         # lstm_out, _ = self.multihead_attn(sentence.view(len(sentence), 1, -1))\n",
    "#         hidden_space = self.linear1(lstm_out.view(len(sentence), -1))\n",
    "#         output = self.linear2(hidden_space.view(-1,len(sentence)))\n",
    "#         output = F.sigmoid(output.squeeze())\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_model = cipher_LSTM(EMBEDDING_DIM, HIDDEN_DIM, max_len)\n",
    "# loss_function = nn.BCELoss()\n",
    "# optimizer = optim.SGD(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "# for i in range(len(train_dataset)):\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     predict = lstm_model(train_dataset[i])\n",
    "#     # print(predict.view(-1,1))\n",
    "#     label = label_list[i]\n",
    "#     label = torch.tensor(label,dtype=torch.float)\n",
    "#     # print(label.view(-1,1))\n",
    "#     loss = loss_function(predict.view(-1), label.view(-1))\n",
    "#     # print(loss.data)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     # if i == 1000:\n",
    "#     #     break\n",
    "# correct = 0\n",
    "# for i in range(100):\n",
    "#     predict = lstm_model(train_dataset[i])\n",
    "    \n",
    "#     if int(torch.round(predict).data)==label_list[i]:\n",
    "#         correct+=1\n",
    "# print(correct)\n",
    "# correct = 0\n",
    "# # for i in range(100):\n",
    "# for i in range(len(dev_dataset)):\n",
    "#     predict = lstm_model(dev_dataset[i])\n",
    "#     # print(predict.data,dev_label_list[i])\n",
    "#     if int(torch.round(predict).data)==dev_label_list[i]:\n",
    "#         correct+=1\n",
    "# print(correct,len(dev_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class selfattention(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim,max_len):\n",
    "        super(selfattention, self).__init__()\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, dim_feedforward=1024, nhead=2)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 1)\n",
    "        self.linear2 = nn.Linear(max_len, 1)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "\n",
    "        encoder_out = self.encoder(sentence.view(len(sentence), 1, -1))\n",
    "\n",
    "        hidden_space = self.linear1(encoder_out.view(len(sentence), -1))\n",
    "        output = self.linear2(hidden_space.view(-1,len(sentence)))\n",
    "        output = F.sigmoid(output.squeeze())\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tommyyu/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "max_len = 100\n",
    "EMBEDDING_DIM = 100\n",
    "selfatten_model = selfattention(EMBEDDING_DIM, max_len)\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.SGD(selfatten_model.parameters(), lr=0.01)\n",
    "loss_list = []\n",
    "for i in range(len(train_dataset)):\n",
    "    optimizer.zero_grad()\n",
    "    predict = selfatten_model(train_dataset[i])\n",
    "    label = label_list[i]\n",
    "    label = torch.tensor(label,dtype=torch.float)\n",
    "    loss = loss_function(predict.view(-1), label.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # if i == 100:\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for sentence in test_dataset:\n",
    "    predict = selfatten_model(sentence)\n",
    "    results.append(int(torch.round(predict.data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct = 0\n",
    "# for i in range(1000):\n",
    "# # for i in range(len(train_dataset)):\n",
    "#     predict = selfatten_model(train_dataset[i])\n",
    "    \n",
    "#     if int(torch.round(predict).data)==label_list[i]:\n",
    "#         correct+=1\n",
    "# print(correct,len(train_dataset))\n",
    "correct = 0\n",
    "for i in range(len(dev_dataset)):\n",
    "    predict = selfatten_model(dev_dataset[i])\n",
    "    \n",
    "    if int(torch.round(predict).data)==dev_label_list[i]:\n",
    "        correct+=1\n",
    "# print(correct/len(dev_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Prediction Result File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to submit a prediction result file. It should have 2028 lines, every line should be either 0 or 1, which is your model's prediction on the respective test set instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose you had your model's predictions on the 2028 test cases read from test_enc_unlabeled.tsv, and \n",
    "#those results are in the list called 'results'\n",
    "assert (len(results) == 2028)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the results are not float numbers, but intergers 0 and 1\n",
    "results = [int(x) for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your prediction results to 'upload_predictions.txt' and upload that later\n",
    "with open('upload_predictions.txt', 'w', encoding = 'utf-8') as fp:\n",
    "    for x in results:\n",
    "        fp.write(str(x) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
